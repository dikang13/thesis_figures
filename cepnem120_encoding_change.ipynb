{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ada18e-718e-4e20-8242-a598a0bf3aae",
   "metadata": {},
   "source": [
    "# Compute encoding change across conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58127e16-823c-429e-af01-938eda639ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JAX_DEBUG_NANS=false\n",
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "env: XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n"
     ]
    }
   ],
   "source": [
    "%env JAX_DEBUG_NANS=false\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "%env XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "\n",
    "# import os\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"   # no big upfront grab\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\"    # or a small fraction\n",
    "# # Optional: better allocator for sharing\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n",
    "\n",
    "\n",
    "# public libraries\n",
    "import jax\n",
    "import jax.tree_util as jtu\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats.norm as norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.special import softmax\n",
    "import h5py\n",
    "import numpy as np\n",
    "from genjax.typing import FloatArray\n",
    "from genjax import ChoiceMap\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "key = jax.random.PRNGKey(314159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8af153-69d5-4a79-b377-f713cdd43a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sourcing files from flv-c servers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/fs/store1/candy/candy_cepnem/private_cepnem_jax/.venv/lib/python3.12/site-packages/jaxopt/__init__.py:59: DeprecationWarning: JAXopt is no longer maintained. See https://docs.jax.dev/en/latest/ for alternatives.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# flavell lab libraries\n",
    "import flv_utils as flv\n",
    "\n",
    "import cepnem_jax as cj\n",
    "from cepnem_jax import (\n",
    "    prob_P_greater_Q,\n",
    "    benjamini_hochberg,\n",
    "    compute_AND,\n",
    "    neuron_p_values,\n",
    "    compute_ζ,\n",
    "    compute_s,\n",
    "    CePNEM_120, \n",
    "    get_CePNEM_data\n",
    ")\n",
    "\n",
    "def compute_α(α0):\n",
    "    return norm.cdf(α0)\n",
    "    \n",
    "def compute_σSE(σ0_SE):\n",
    "    return σ_SE_MEAN * jnp.exp(σ0_SE * σ_SE_STD)\n",
    "\n",
    "def compute_σnoise(σ0_noise):\n",
    "    return σ_NOISE_MEAN * jnp.exp(σ0_noise * σ_NOISE_STD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae66d224-1755-47b0-b18c-0ea1f810255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def effective_gains(c_vT, c_v, c_th, c_P, s):\n",
    "    \"\"\"Compute 6 effective gains (v, θh, P for v>=0 and v<0).\"\"\"\n",
    "    denom = jnp.sqrt(c_vT**2 + 1.0)\n",
    "    r_pos = (c_vT + 1.0) / denom\n",
    "    r_neg = (1.0 - c_vT) / denom\n",
    "\n",
    "    g_v  = c_v  / (s + 1.0)\n",
    "    g_th = c_th / (s + 1.0)\n",
    "    g_P  = c_P  / (s + 1.0)\n",
    "\n",
    "    g_pos = jnp.array([r_pos * g_v, r_pos * g_th, r_pos * g_P])\n",
    "    g_neg = jnp.array([r_neg * g_v, r_neg * g_th, r_neg * g_P])\n",
    "\n",
    "    return jnp.concatenate([g_pos, g_neg])  # shape (6,)\n",
    "\n",
    "\n",
    "def effective_gains_from_posterior(posterior: dict):\n",
    "    \"\"\"\n",
    "    Compute effective gains for all posterior samples.\n",
    "    \n",
    "    Args:\n",
    "        posterior: dict with keys \"c_vT\", \"c_v\", \"c_theta_h\", \"c_P\", \"s\".\n",
    "                   Each value is an array of shape (n_samples,) or (n_chains, n_samples).\n",
    "    \n",
    "    Returns:\n",
    "        Array of shape (..., 6) containing effective gains per sample.\n",
    "    \"\"\"\n",
    "    c_vT = posterior[\"c_vT\"].reshape(-1)\n",
    "    c_v  = posterior[\"c_v\"].reshape(-1)\n",
    "    c_th = posterior[\"c_θh\"].reshape(-1)\n",
    "    c_P  = posterior[\"c_P\"].reshape(-1)\n",
    "\n",
    "    if \"s\" in posterior.keys():\n",
    "        s = posterior[\"s\"].reshape(-1)\n",
    "    else:\n",
    "        s = compute_s(posterior[\"s0\"]).reshape(-1)\n",
    "        \n",
    "    # Vectorize over all leading dimensions of the arrays\n",
    "    batched_effective_gains = jax.vmap(effective_gains)\n",
    "    return batched_effective_gains(c_vT, c_v, c_th, c_P, s)\n",
    "\n",
    "\n",
    "# # Test case\n",
    "# posterior = {\n",
    "#     \"c_vT\": jnp.array([0.5, -1.2]),\n",
    "#     \"c_v\":  jnp.array([1.0, 0.8]),\n",
    "#     \"c_θh\": jnp.array([0.3, 0.5]),\n",
    "#     \"c_P\":  jnp.array([2.1, 1.9]),\n",
    "#     \"s\":    jnp.array([5.0, 4.0])\n",
    "# }\n",
    "\n",
    "# gains = effective_gains_from_posterior(posterior)\n",
    "# print(gains.shape)  # (2, 6)\n",
    "# print(gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14550d54-0e61-4be8-9c7c-13e65e8f1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(distribution_0, distribution_1):\n",
    "\n",
    "    len_0 = len(distribution_0)\n",
    "    len_1 = len(distribution_1)\n",
    "    \n",
    "    if len_0 == 1:\n",
    "        is_greater = distribution_0[0] < distribution_1\n",
    "        is_smaller = distribution_0[0] > distribution_1\n",
    "        n = len_1\n",
    "        \n",
    "    elif len_1 == 1:\n",
    "        is_greater = distribution_0 < distribution_1[0]\n",
    "        is_smaller = distribution_0 > distribution_1[0]        \n",
    "        n = len_0\n",
    "        \n",
    "    else:\n",
    "        n = np.min([len_0, len_1])\n",
    "        is_greater = distribution_0[:n] < distribution_1[:n]\n",
    "        is_smaller = distribution_0[:n] > distribution_1[:n]\n",
    "        \n",
    "    p_greater = 1 - sum(is_greater) / n\n",
    "    p_smaller = 1 - sum(is_smaller) / n\n",
    "    \n",
    "    return np.min([p_greater, p_smaller])\n",
    "\n",
    "\n",
    "def benjamini_hochberg_correction(p_values):\n",
    "    \"\"\"\n",
    "    Apply Benjamini-Hochberg correction to control false discovery rate (FDR).\n",
    "    \n",
    "    Parameters:\n",
    "    p_values: array-like, dict, or nested dict of p-values\n",
    "    \n",
    "    Returns:\n",
    "    corrected_p_values: same structure as input, containing BH-corrected p-values\n",
    "    \"\"\"\n",
    "    # Handle dictionary inputs\n",
    "    if isinstance(p_values, dict):\n",
    "        return _bh_correction_dict(p_values)\n",
    "    \n",
    "    # Handle array-like inputs\n",
    "    return _bh_correction_array(p_values)\n",
    "\n",
    "def _bh_correction_dict(p_dict):\n",
    "    \"\"\"\n",
    "    Handle dictionary and nested dictionary inputs.\n",
    "    \"\"\"\n",
    "    # Collect all p-values and their paths\n",
    "    all_p_values = []\n",
    "    paths = []\n",
    "    \n",
    "    def collect_values(d, current_path=\"\"):\n",
    "        for key, value in d.items():\n",
    "            new_path = f\"{current_path}.{key}\" if current_path else str(key)\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                # Nested dictionary - recurse\n",
    "                collect_values(value, new_path)\n",
    "            elif isinstance(value, (int, float, np.number)):\n",
    "                # Single p-value\n",
    "                all_p_values.append(float(value))\n",
    "                paths.append(new_path)\n",
    "            elif hasattr(value, '__iter__') and not isinstance(value, str):\n",
    "                # Array-like value\n",
    "                value_array = np.array(value).flatten()\n",
    "                for i, v in enumerate(value_array):\n",
    "                    all_p_values.append(float(v))\n",
    "                    paths.append(f\"{new_path}[{i}]\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported value type at {new_path}: {type(value)}\")\n",
    "    \n",
    "    collect_values(p_dict)\n",
    "    \n",
    "    # Apply BH correction to all collected p-values\n",
    "    corrected_flat = _bh_correction_array(np.array(all_p_values))\n",
    "    \n",
    "    # Reconstruct the original structure with corrected values\n",
    "    corrected_dict = {}\n",
    "    path_to_corrected = dict(zip(paths, corrected_flat))\n",
    "    \n",
    "    def reconstruct_dict(d, current_path=\"\"):\n",
    "        result = {}\n",
    "        for key, value in d.items():\n",
    "            new_path = f\"{current_path}.{key}\" if current_path else str(key)\n",
    "            \n",
    "            if isinstance(value, dict):\n",
    "                # Nested dictionary\n",
    "                result[key] = reconstruct_dict(value, new_path)\n",
    "            elif isinstance(value, (int, float, np.number)):\n",
    "                # Single p-value\n",
    "                result[key] = path_to_corrected[new_path]\n",
    "            elif hasattr(value, '__iter__') and not isinstance(value, str):\n",
    "                # Array-like value - reconstruct original shape\n",
    "                original_array = np.array(value)\n",
    "                flat_size = original_array.size\n",
    "                corrected_values = [path_to_corrected[f\"{new_path}[{i}]\"] \n",
    "                                  for i in range(flat_size)]\n",
    "                result[key] = np.array(corrected_values).reshape(original_array.shape)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return reconstruct_dict(p_dict)\n",
    "\n",
    "def _bh_correction_array(p_values):\n",
    "    \"\"\"\n",
    "    Apply BH correction to array-like p-values.\n",
    "    \"\"\"\n",
    "    # Convert to numpy array and store original shape\n",
    "    p_array = np.array(p_values)\n",
    "    original_shape = p_array.shape\n",
    "    \n",
    "    # Flatten for processing\n",
    "    p_flat = p_array.flatten()\n",
    "    n = len(p_flat)\n",
    "    \n",
    "    if n == 0:\n",
    "        return p_array\n",
    "    \n",
    "    # Get sorting indices (ascending order)\n",
    "    sorted_indices = np.argsort(p_flat)\n",
    "    \n",
    "    # Sort p-values\n",
    "    p_sorted = p_flat[sorted_indices]\n",
    "    \n",
    "    # Calculate BH correction: p_adjusted = p * n / rank\n",
    "    # rank goes from 1 to n (not 0 to n-1)\n",
    "    ranks = np.arange(1, n + 1)\n",
    "    p_adjusted = p_sorted * n / ranks\n",
    "    \n",
    "    # Ensure monotonicity (corrected p-values should be non-decreasing)\n",
    "    # Work backwards through the sorted array\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        if p_adjusted[i] > p_adjusted[i + 1]:\n",
    "            p_adjusted[i] = p_adjusted[i + 1]\n",
    "    \n",
    "    # Cap at 1.0 (p-values cannot exceed 1)\n",
    "    p_adjusted = np.minimum(p_adjusted, 1.0)\n",
    "    \n",
    "    # Create output array and restore original order\n",
    "    corrected_p_values = np.empty_like(p_flat)\n",
    "    corrected_p_values[sorted_indices] = p_adjusted\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    return corrected_p_values.reshape(original_shape)\n",
    "\n",
    "def array_to_dict_with_indices(arr):\n",
    "    \"\"\"\n",
    "    Convert an array to a dictionary where keys correspond to array indices.\n",
    "    Supports 1D, 2D, 3D, and higher dimensional arrays.\n",
    "    \"\"\"\n",
    "    arr = np.array(arr)\n",
    "    \n",
    "    if arr.ndim == 1:\n",
    "        # 1D array: {0: val0, 1: val1, ...}\n",
    "        return {i: arr[i] for i in range(len(arr))}\n",
    "    \n",
    "    elif arr.ndim == 2:\n",
    "        # 2D array: {0: {0: val00, 1: val01}, 1: {0: val10, 1: val11}, ...}\n",
    "        return {i: {j: arr[i, j] for j in range(arr.shape[1])} \n",
    "                for i in range(arr.shape[0])}\n",
    "    \n",
    "    elif arr.ndim == 3:\n",
    "        # 3D array: {0: {0: {0: val000, 1: val001}, 1: {0: val010, 1: val011}}, ...}\n",
    "        return {i: {j: {k: arr[i, j, k] for k in range(arr.shape[2])} \n",
    "                    for j in range(arr.shape[1])} \n",
    "                for i in range(arr.shape[0])}\n",
    "    \n",
    "    else:\n",
    "        # Higher dimensions - flatten to 1D with multi-index keys\n",
    "        flat_arr = arr.flatten()\n",
    "        result = {}\n",
    "        for idx, val in enumerate(flat_arr):\n",
    "            multi_idx = np.unravel_index(idx, arr.shape)\n",
    "            # Create nested dict structure\n",
    "            current_dict = result\n",
    "            for dim_idx in multi_idx[:-1]:\n",
    "                if dim_idx not in current_dict:\n",
    "                    current_dict[dim_idx] = {}\n",
    "                current_dict = current_dict[dim_idx]\n",
    "            current_dict[multi_idx[-1]] = val\n",
    "        \n",
    "        return result\n",
    "\n",
    "def dict_to_array_values(d, original_shape):\n",
    "    \"\"\"\n",
    "    Extract values from nested dictionary back to array format.\n",
    "    \"\"\"\n",
    "    def extract_values(nested_dict):\n",
    "        if isinstance(nested_dict, dict):\n",
    "            values = []\n",
    "            for key in sorted(nested_dict.keys()):\n",
    "                values.extend(extract_values(nested_dict[key]))\n",
    "            return values\n",
    "        else:\n",
    "            return [nested_dict]\n",
    "    \n",
    "    flat_values = extract_values(d)\n",
    "    return np.array(flat_values).reshape(original_shape)\n",
    "\n",
    "def run_test_case(test_name, array_data):\n",
    "    \"\"\"\n",
    "    Run a test case comparing array vs dictionary BH correction.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST: {test_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Convert array to dictionary\n",
    "    dict_data = array_to_dict_with_indices(array_data)\n",
    "    \n",
    "    print(f\"Original array shape: {np.array(array_data).shape}\")\n",
    "    print(f\"Original array:\\n{np.array(array_data)}\")\n",
    "    print(f\"\\nCorresponding dictionary:\")\n",
    "    print(dict_data)\n",
    "    \n",
    "    # Apply BH correction to both\n",
    "    corrected_array = benjamini_hochberg_correction(array_data)\n",
    "    corrected_dict = benjamini_hochberg_correction(dict_data)\n",
    "    \n",
    "    # Convert dictionary results back to array for comparison\n",
    "    dict_as_array = dict_to_array_values(corrected_dict, corrected_array.shape)\n",
    "    \n",
    "    print(f\"\\nBH corrected array:\\n{corrected_array}\")\n",
    "    print(f\"\\nBH corrected dictionary values as array:\\n{dict_as_array}\")\n",
    "    \n",
    "    # Check if they're equal\n",
    "    are_equal = np.allclose(corrected_array, dict_as_array, rtol=1e-10, atol=1e-15)\n",
    "    print(f\"\\nResults are {'IDENTICAL' if are_equal else 'DIFFERENT'}\")\n",
    "    \n",
    "    if not are_equal:\n",
    "        print(f\"Maximum difference: {np.max(np.abs(corrected_array - dict_as_array))}\")\n",
    "    \n",
    "    return are_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a3ba17-e8c9-486b-8279-3b1fee247b39",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e00f2d-2628-4d6f-89ca-b19616bfb558",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conditions = ['dense_food', 'sparse_food', 'just_fed', 'fasted', 'starved', 'octanol', 'gfp', 'scrambled'] #'diacetyl', 'heat_stim', \n",
    "\n",
    "dict_color = dict()\n",
    "dict_color['dense_food']  = 'saddlebrown'\n",
    "dict_color['sparse_food'] = 'darkorange'\n",
    "dict_color['diacetyl']    = 'deeppink'\n",
    "dict_color['just_fed']    = 'dodgerblue'\n",
    "dict_color['fasted']      = 'lightseagreen'\n",
    "dict_color['starved']     = 'darkgreen'\n",
    "dict_color['octanol']     = 'navy'\n",
    "dict_color['heat_stim']   = 'darkred'\n",
    "dict_color['gfp']         = 'lightgrey'\n",
    "dict_color['scrambled']   = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cd51e-9a8c-41ef-80e0-41e68407ee46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIA not found\n",
      "AVL not found\n",
      "RIF not found\n",
      "AVD not found for dense_food\n",
      "AVD not found for starved\n",
      "AVD not found for octanol\n",
      "RIR not found\n",
      "RIP not found\n",
      "SAADR not found\n",
      "SMBD not found\n",
      "SIAV not found\n",
      "SIAD not found\n",
      "SIBV not found\n",
      "SIBD not found\n"
     ]
    }
   ],
   "source": [
    "# nc = 'AVA'\n",
    "full_model_dir = '/store1/candy/candy_cepnem/candy_testing/om_fits_120/2025-08-25'\n",
    "control = 'scrambled'\n",
    "# list_nc = ['AVA', 'AVB', 'RIB', 'MC', 'RMG']\n",
    "\n",
    "# g_idx = 3  # reversal gain\n",
    "dict_G = dict()\n",
    "dict_p_control = dict()\n",
    "\n",
    "for nc in flv.fig4_neuron_classes:\n",
    "    if not os.path.isdir(f'{full_model_dir}/{nc}'):\n",
    "        print(f'{nc} not found')\n",
    "        continue\n",
    "    \n",
    "    dict_G[nc] = dict()\n",
    "    dict_p_control[nc] = dict()\n",
    "\n",
    "    # compute effective gains from control condition\n",
    "    path_output = f'{full_model_dir}/{nc}/all.h5'\n",
    "    result = flv.h5_to_dict(path_output)\n",
    "    output = result['CePNEMOutput']\n",
    "    posterior = output['posterior_samples']\n",
    "    dict_G[nc][control] = effective_gains_from_posterior(posterior)    \n",
    "    \n",
    "    for condition in all_conditions:\n",
    "        if condition is not control:\n",
    "            path_output = f'{full_model_dir}/{nc}/{condition}.h5'\n",
    "            if not os.path.isfile(path_output):\n",
    "                print(f'{nc} not found for {condition}')\n",
    "                continue\n",
    "            \n",
    "            result = flv.h5_to_dict(path_output)\n",
    "            output = result['CePNEMOutput']\n",
    "            posterior = output['posterior_samples']\n",
    "            dict_G[nc][condition] = effective_gains_from_posterior(posterior)\n",
    "\n",
    "#             dict_p_control[nc][condition] = compare_distributions(dict_G[nc][control][:,g_idx], \n",
    "#                                                                   dict_G[nc][condition][:,g_idx])\n",
    "\n",
    "# dict_p_control_corrected = benjamini_hochberg_correction(dict_p_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b011989-387b-48ca-be3d-30ec15f9ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots (3 rows, 3 columns) with shared x-axis per column\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 8), sharex='col')\n",
    "\n",
    "# Neuron class\n",
    "nc = 'RMG'\n",
    "\n",
    "# Define the xlabel order\n",
    "xlabels = ['g_v (+)', 'g_θh (+)', 'g_P (+)', 'g_v (-)', 'g_θh (-)', 'g_P (-)']\n",
    "\n",
    "# Loop through each g_idx for the first 6 subplots\n",
    "for plot_idx, g_idx in enumerate(range(6)):\n",
    "    row = plot_idx // 3\n",
    "    col = plot_idx % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Plot histograms for each condition\n",
    "    for condition in all_conditions:\n",
    "        ax.hist(dict_G[nc][condition][:,g_idx], \n",
    "                color=dict_color[condition], alpha=0.7, \n",
    "                label=f\"{condition}\")\n",
    "    \n",
    "    # Set labels for each subplot\n",
    "    ax.set_xlabel(xlabels[plot_idx], fontweight='bold', fontsize=12)\n",
    "    if plot_idx == 0:\n",
    "        ax.set_ylabel('Posterior sample counts', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add sum distributions in the third row\n",
    "sum_labels = ['g_v (+) + g_v (-)', 'g_θh (+) + g_θh (-)', 'g_P (+) + g_P (-)']\n",
    "sum_pairs = [(0, 3), (1, 4), (2, 5)]  # Pairs of indices to sum\n",
    "\n",
    "for sum_idx, (idx1, idx2) in enumerate(sum_pairs):\n",
    "    ax = axes[2, sum_idx]  # Third row subplots\n",
    "    \n",
    "    # Plot histograms for each condition\n",
    "    for condition in all_conditions:\n",
    "        # Sum the corresponding positive and negative components\n",
    "        sum_data = dict_G[nc][condition][:,idx1] + dict_G[nc][condition][:,idx2]\n",
    "        ax.hist(sum_data, \n",
    "                color=dict_color[condition], alpha=0.7, \n",
    "                label=f\"{condition}\")\n",
    "    \n",
    "    # Set labels for sum subplots\n",
    "    ax.set_xlabel(sum_labels[sum_idx], fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add legend only to the first subplot, positioned outside\n",
    "legend = axes[0, 2].legend(bbox_to_anchor=(1, 0.97), loc='right')\n",
    "# Set legend alpha to 1 (fully opaque)\n",
    "for patch in legend.get_patches():\n",
    "    patch.set_alpha(1)\n",
    "\n",
    "# Adjust layout to prevent legend cutoff\n",
    "plt.tight_layout()\n",
    "# Add extra space on the right for legends\n",
    "# plt.subplots_adjust(right=0.95)\n",
    "\n",
    "# Overall title for the entire figure\n",
    "fig.suptitle(f'Effective neural gain for {nc} as a function of behaviors', \n",
    "             fontweight='bold', fontsize=20, y=0.98)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a0222-01c2-4c50-bd10-bbddb798d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a list of neuron classes\n",
    "neuron_classes = flv.fig4_neuron_classes\n",
    "\n",
    "# Create figure for single heatmap\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "# Initialize matrices to store mean values for heatmap\n",
    "# Rows: conditions, Columns: neuron classes\n",
    "n_conditions = len(all_conditions)\n",
    "n_nc = len(neuron_classes)\n",
    "\n",
    "gv_pos_matrix = np.zeros((n_conditions, n_nc))\n",
    "gv_neg_matrix = np.zeros((n_conditions, n_nc))\n",
    "\n",
    "# Fill the matrices with mean values\n",
    "for nc_idx, nc in enumerate(neuron_classes):\n",
    "    for cond_idx, condition in enumerate(all_conditions):\n",
    "        # g_v(+) is at index 0, g_v(-) is at index 3\n",
    "        gv_pos_matrix[cond_idx, nc_idx] = np.mean(dict_G[nc][condition][:, 0])\n",
    "        gv_neg_matrix[cond_idx, nc_idx] = np.mean(dict_G[nc][condition][:, 3])\n",
    "\n",
    "# Create sum matrix\n",
    "gv_sum_matrix = gv_pos_matrix + gv_neg_matrix\n",
    "\n",
    "# Create custom colormap with white for NaN values\n",
    "cmap = plt.cm.coolwarm.copy()\n",
    "cmap.set_bad(color='white')\n",
    "\n",
    "# Create single heatmap\n",
    "im = ax.imshow(combined_matrix, cmap=cmap, aspect='equal', vmin=-0.25, vmax=0.25)\n",
    "\n",
    "# Set title\n",
    "ax.set_title('g_v (+) + g_v (-)', fontweight='bold', fontsize=16)\n",
    "\n",
    "# Set x-axis labels (neuron classes)\n",
    "ax.set_xticks(range(n_nc))\n",
    "ax.set_xticklabels(neuron_classes)\n",
    "ax.set_xlabel('Neuron Class', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Set y-axis labels (conditions)\n",
    "ax.set_yticks(range(n_conditions))\n",
    "ax.set_yticklabels(all_conditions)\n",
    "ax.set_ylabel('Condition', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Mean g_v (+) + g_v (-)', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add value annotations on heatmap\n",
    "for i in range(n_conditions):\n",
    "    for j in range(n_nc):\n",
    "        ax.text(j, i, f'{gv_sum_matrix[i, j]:.3f}', \n",
    "                ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Overall title\n",
    "fig.suptitle('Effective neural gain g_v sum across neuron classes and conditions', \n",
    "             fontweight='bold', fontsize=18, y=0.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Alternative: Single heatmap with g_v(+) and g_v(-) as consecutive row pairs per condition\n",
    "fig2, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Create matrix with gaps: (2 * n_conditions + gaps) x n_nc\n",
    "# We'll add a gap row between each condition pair\n",
    "gap_size = 1\n",
    "total_rows = 2 * n_conditions + (n_conditions - 1) * gap_size\n",
    "combined_matrix = np.full((total_rows, n_nc), np.nan)  # Use NaN for gaps\n",
    "\n",
    "# Fill the matrix with g_v values\n",
    "row_labels = []\n",
    "row_positions = []\n",
    "current_row = 0\n",
    "\n",
    "for cond_idx, condition in enumerate(all_conditions):\n",
    "    # Add g_v(+) row\n",
    "    for nc_idx, nc in enumerate(neuron_classes):\n",
    "        combined_matrix[current_row, nc_idx] = np.mean(dict_G[nc][condition][:, 0])  # g_v(+)\n",
    "    row_labels.append(f'{condition} g_v(+)')\n",
    "    row_positions.append(current_row)\n",
    "    current_row += 1\n",
    "    \n",
    "    # Add g_v(-) row\n",
    "    for nc_idx, nc in enumerate(neuron_classes):\n",
    "        combined_matrix[current_row, nc_idx] = np.mean(dict_G[nc][condition][:, 3])  # g_v(-)\n",
    "    row_labels.append(f'{condition} g_v(-)')\n",
    "    row_positions.append(current_row)\n",
    "    current_row += 1\n",
    "    \n",
    "    # Add gap row (except after the last condition)\n",
    "    if cond_idx < n_conditions - 1:\n",
    "        current_row += gap_size\n",
    "\n",
    "# Create custom colormap with white for NaN values\n",
    "cmap = plt.cm.coolwarm.copy()\n",
    "cmap.set_bad(color='white')\n",
    "\n",
    "im = ax.imshow(combined_matrix, cmap=cmap, aspect='equal', vmin=-0.25, vmax=0.25)\n",
    "\n",
    "# Set x-axis labels (neuron classes)\n",
    "ax.set_xticks(range(n_nc))\n",
    "ax.set_xticklabels(neuron_classes)\n",
    "ax.set_xlabel('Neuron Class', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Set y-axis labels (only for actual data rows, not gap rows)\n",
    "ax.set_yticks(row_positions)\n",
    "ax.set_yticklabels(row_labels)\n",
    "ax.set_ylabel('Condition and g_v type', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Mean g_v', fontweight='bold', fontsize=12)\n",
    "\n",
    "# # Add value annotations (only for non-NaN values)\n",
    "# for i in range(total_rows):\n",
    "#     for j in range(n_nc):\n",
    "#         if not np.isnan(combined_matrix[i, j]):\n",
    "#             ax.text(j, i, f'{combined_matrix[i, j]:.3f}', \n",
    "#                     ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "# Add horizontal lines to separate condition groups\n",
    "for cond_idx in range(n_conditions - 1):\n",
    "    y_position = 2 * (cond_idx + 1) + cond_idx * gap_size - 0.5\n",
    "    ax.axhline(y=y_position, color='white', linewidth=3)\n",
    "\n",
    "plt.title('Effective neural gain g_v across neuron classes and conditions', \n",
    "          fontweight='bold', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e899a85-486e-4d8a-8c6a-dd1641614fed",
   "metadata": {},
   "source": [
    "## Compute a single metric from the 6 effective gain (transformed CePNEM parameters) per condition per neuron class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573a12e-6f1d-4f5e-88fb-078263c2922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional SciPy speed-up for triangular solves (falls back to NumPy if missing)\n",
    "try:\n",
    "    from scipy.linalg import solve_triangular as _tri_solve\n",
    "    def tri_solve(L, B):  # L is lower-triangular, solve L Y = B\n",
    "        return _tri_solve(L, B, lower=True, check_finite=False, overwrite_b=False)\n",
    "except Exception:\n",
    "    def tri_solve(L, B):\n",
    "        # Generic solver; fine at this 6x6 size\n",
    "        return np.linalg.solve(L, B)\n",
    "\n",
    "def _clean_rows(X):\n",
    "    \"\"\"Keep only rows that are fully finite.\"\"\"\n",
    "    m = np.isfinite(X).all(axis=1)\n",
    "    return X[m], m\n",
    "\n",
    "def _cholesky_with_adaptive_ridge(S, ridge0=1e-6, max_tries=6):\n",
    "    \"\"\"\n",
    "    Ensure PD by adding ridge*I; if Cholesky fails, grow ridge by x10 up to max_tries.\n",
    "    Returns (L, used_ridge) with S_reg = L @ L.T.\n",
    "    \"\"\"\n",
    "    I = np.eye(S.shape[0], dtype=S.dtype)\n",
    "    ridge = ridge0\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            L = np.linalg.cholesky(S + ridge * I)\n",
    "            return L, ridge\n",
    "        except np.linalg.LinAlgError:\n",
    "            ridge *= 10.0\n",
    "    # Final attempt with a big ridge so we fail loudly if something is truly wrong\n",
    "    L = np.linalg.cholesky(S + ridge * I)\n",
    "    return L, ridge\n",
    "\n",
    "def fit_ref_params_cpu(X_ref, shrink=0.0, ridge=1e-6):\n",
    "    \"\"\"\n",
    "    Estimate reference mean and a PD covariance (via shrink + adaptive ridge).\n",
    "    X_ref: (n_ref, 6) numpy array (CPU)\n",
    "    shrink: 0..1 diagonal shrinkage toward diag(S)\n",
    "    ridge: initial ridge; will be increased automatically if needed\n",
    "    Returns:\n",
    "        mu_ref: (6,)\n",
    "        L: lower Cholesky factor of Σ_ref (so Σ_ref = L @ L.T)\n",
    "        used_ridge: ridge actually applied after adaptation\n",
    "    \"\"\"\n",
    "    X_ref = np.asarray(X_ref)\n",
    "    X_ref, mask = _clean_rows(X_ref)\n",
    "    if X_ref.shape[0] < 2:\n",
    "        raise ValueError(\"Reference has <2 finite rows after cleaning; cannot estimate covariance.\")\n",
    "\n",
    "    mu = X_ref.mean(axis=0)\n",
    "    X0 = X_ref - mu\n",
    "    n = X_ref.shape[0]\n",
    "    # Unbiased sample covariance\n",
    "    S = (X0.T @ X0) / max(n - 1, 1)\n",
    "\n",
    "    if shrink > 0.0:\n",
    "        S = (1.0 - shrink) * S + shrink * np.diag(np.diag(S))\n",
    "\n",
    "    # Scale ridge to data units (helps numeric stability)\n",
    "    avg_var = np.trace(S) / S.shape[0]\n",
    "    S = S + (ridge * (avg_var + 1.0)) * np.eye(S.shape[0], dtype=S.dtype)\n",
    "\n",
    "    L, used_ridge = _cholesky_with_adaptive_ridge(S, ridge0=ridge)\n",
    "    return mu, L, used_ridge\n",
    "\n",
    "def mahalanobis_given_params_cpu(X_target, mu_ref, L, squared=False):\n",
    "    \"\"\"\n",
    "    Compute Mahalanobis distances for target samples given reference (mu_ref, L).\n",
    "    X_target: (n_t, 6) numpy array\n",
    "    mu_ref:   (6,)\n",
    "    L:        (6,6) lower Cholesky of Σ_ref\n",
    "    \"\"\"\n",
    "    X_target = np.asarray(X_target)\n",
    "    diff = X_target - mu_ref                      # (n_t, 6)\n",
    "    # Solve L Y = diff^T  -> Y = L^{-1} diff^T\n",
    "    Y = tri_solve(L, diff.T)                      # (6, n_t)\n",
    "    d2 = np.einsum('ij,ij->j', Y, Y)              # (n_t,)\n",
    "    return d2 if squared else np.sqrt(np.maximum(d2, 0.0))\n",
    "\n",
    "def mahalanobis_from_reference_cpu(X_target, X_ref, shrink=0.0, ridge=1e-6, squared=False, return_params=False):\n",
    "    \"\"\"\n",
    "    Convenience wrapper: fit reference params on CPU, then compute distances.\n",
    "    \"\"\"\n",
    "    mu_ref, L, used_ridge = fit_ref_params_cpu(X_ref, shrink=shrink, ridge=ridge)\n",
    "    d = mahalanobis_given_params_cpu(X_target, mu_ref, L, squared=squared)\n",
    "    if return_params:\n",
    "        return d, mu_ref, L, used_ridge\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2288ed-e2cd-4090-b029-7e453ae4b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per neuron, per condition\n",
    "# X_cond, X_ctrl are jnp arrays shaped (n_samples, 6)\n",
    "X_cond = dict_G['AVA']['sparse_food']\n",
    "X_ctrl = dict_G['AVA']['scrambled']\n",
    "\n",
    "X_ctrl_np = np.asarray(X_ctrl)\n",
    "X_cond_np = np.asarray(X_cond)\n",
    "\n",
    "d_cond_vs_ctrl = mahalanobis_from_reference_cpu(\n",
    "    X_target=X_cond_np,   # (n_samples, 6)\n",
    "    X_ref=X_ctrl_np,      # (n_samples, 6)\n",
    "    shrink=0.1,           # light diagonal shrinkage (helps if metrics are collinear)\n",
    "    ridge=1e-6,           # starting ridge; will increase automatically if needed\n",
    "    squared=False\n",
    ")  # -> (n_samples,)\n",
    "\n",
    "plt.hist(d_cond_vs_ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102a587-6018-4650-95e9-bfdfc1d722b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(d_cond_vs_ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41621517-2c17-4cf7-9ac5-e638c3d25344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per neuron, per condition\n",
    "# X_cond, X_ctrl are jnp arrays shaped (n_samples, 6)\n",
    "X_cond = dict_G['AVA']['gfp']\n",
    "X_ctrl = dict_G['AVA']['scrambled']\n",
    "\n",
    "X_ctrl_np = np.asarray(X_ctrl)\n",
    "X_cond_np = np.asarray(X_cond)\n",
    "\n",
    "d_cond_vs_ctrl = mahalanobis_from_reference_cpu(\n",
    "    X_target=X_cond_np,   # (n_samples, 6)\n",
    "    X_ref=X_ctrl_np,      # (n_samples, 6)\n",
    "    shrink=0.1,           # light diagonal shrinkage (helps if metrics are collinear)\n",
    "    ridge=1e-6,           # starting ridge; will increase automatically if needed\n",
    "    squared=False\n",
    ")  # -> (n_samples,)\n",
    "\n",
    "plt.hist(d_cond_vs_ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e81011-5da1-42f1-87a1-42ea67b91672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per neuron, per condition\n",
    "# X_cond, X_ctrl are jnp arrays shaped (n_samples, 6)\n",
    "X_cond = dict_G['AVA']['dense_food']\n",
    "X_ctrl = dict_G['AVA']['sparse_food']\n",
    "\n",
    "X_ctrl_np = np.asarray(X_ctrl)\n",
    "X_cond_np = np.asarray(X_cond)\n",
    "\n",
    "d_cond_vs_ctrl = mahalanobis_from_reference_cpu(\n",
    "    X_target=X_cond_np,   # (n_samples, 6)\n",
    "    X_ref=X_ctrl_np,      # (n_samples, 6)\n",
    "    shrink=0.1,           # light diagonal shrinkage (helps if metrics are collinear)\n",
    "    ridge=1e-6,           # starting ridge; will increase automatically if needed\n",
    "    squared=False\n",
    ")  # -> (n_samples,)\n",
    "\n",
    "plt.hist(d_cond_vs_ctrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc73d2-4491-4663-97c4-5e0c8037fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Optional, Sequence, Union, Tuple\n",
    "\n",
    "def variability_score_location_only(\n",
    "    data_by_cond: Dict[str, np.ndarray],\n",
    "    ignore_conditions: Optional[Sequence[str]] = None,\n",
    "    metrics: Optional[Sequence[int]] = None,\n",
    "    scale: bool = True,\n",
    "    reg: Optional[float] = None,\n",
    "    return_diagnostics: bool = False,\n",
    ") -> Union[float, Tuple[float, dict]]:\n",
    "    \"\"\"\n",
    "    Location-only variability score across multiple conditions.\n",
    "\n",
    "    The score reflects how different the *centers* (group means) are relative to a pooled\n",
    "    within-group scatter matrix. Differences in spread alone (with overlapping centers)\n",
    "    do not increase the score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_by_cond : dict[str, (n_obs, n_metrics) array]\n",
    "        Mapping from condition name to data matrix.\n",
    "    ignore_conditions : sequence[str], optional\n",
    "        Conditions to exclude.\n",
    "    metrics : sequence[int], optional\n",
    "        Column indices to include (e.g., [0,2,5]). If None, use all columns.\n",
    "    scale : bool, default True\n",
    "        If True, z-score each included metric across *all* samples (prevents dominance by scale).\n",
    "    reg : float, optional\n",
    "        Ridge added to pooled within scatter S_W for numerical stability.\n",
    "        If None, uses adaptive ridge = 1e-8 * trace(S_W)/d (d = #included metrics).\n",
    "    return_diagnostics : bool, default False\n",
    "        If True, also return a dict with S_B, S_W, J, per-group means, etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float in [0,1]\n",
    "        Normalized Hotelling–Lawley trace: J / (1 + J).\n",
    "    (score, diagnostics) if return_diagnostics=True\n",
    "    \"\"\"\n",
    "\n",
    "    # --- gather and subset ---\n",
    "    ignore = set(ignore_conditions or [])\n",
    "    groups = {}\n",
    "    for k, X in data_by_cond.items():\n",
    "        if k in ignore:\n",
    "            continue\n",
    "        X = np.asarray(X)\n",
    "        if X.ndim != 2 or X.shape[0] < 2:\n",
    "            continue\n",
    "        if metrics is not None:\n",
    "            X = X[:, metrics]\n",
    "        # drop rows with any NaN (in included metrics)\n",
    "        mask = ~np.isnan(X).any(axis=1)\n",
    "        if mask.sum() >= 2 and X.shape[1] >= 1:\n",
    "            groups[k] = X[mask]\n",
    "\n",
    "    if len(groups) < 2:\n",
    "        raise ValueError(\"Need at least two usable conditions after filtering.\")\n",
    "    d = next(iter(groups.values())).shape[1]\n",
    "\n",
    "    # --- optional global scaling (on included metrics only) ---\n",
    "    if scale:\n",
    "        all_data = np.vstack(list(groups.values()))\n",
    "        mean_all = all_data.mean(axis=0)\n",
    "        std_all  = all_data.std(axis=0, ddof=1)\n",
    "        std_all[std_all == 0] = 1.0\n",
    "        for k in groups:\n",
    "            groups[k] = (groups[k] - mean_all) / std_all\n",
    "\n",
    "    # --- per-condition means & covariances ---\n",
    "    mus, Sigmas, ns = {}, {}, {}\n",
    "    for k, X in groups.items():\n",
    "        n_i = X.shape[0]\n",
    "        mu_i = X.mean(axis=0)\n",
    "        Xi = X - mu_i\n",
    "        # unbiased covariance (works for d=1 as well)\n",
    "        Sigma_i = (Xi.T @ Xi) / (n_i - 1)\n",
    "        mus[k], Sigmas[k], ns[k] = mu_i, np.atleast_2d(Sigma_i), n_i\n",
    "\n",
    "    N = sum(ns.values())\n",
    "    mu_bar = sum(ns[k] * mus[k] for k in groups) / N\n",
    "\n",
    "    # --- between scatter (means only) ---\n",
    "    S_B = np.zeros((d, d))\n",
    "    for k in groups:\n",
    "        diff = (mus[k] - mu_bar)[:, None]\n",
    "        S_B += ns[k] * (diff @ diff.T)\n",
    "\n",
    "    # --- pooled within scatter ---\n",
    "    S_W = np.zeros((d, d))\n",
    "    for k in groups:\n",
    "        S_W += (ns[k] - 1) * Sigmas[k]\n",
    "\n",
    "    # --- regularize and compute score ---\n",
    "    eps = (1e-8 * (np.trace(S_W) / max(d, 1))) if reg is None else float(reg)\n",
    "    S_W_reg = S_W + eps * np.eye(d)\n",
    "\n",
    "    # Hotelling–Lawley trace J = tr(S_W^{-1} S_B)\n",
    "    J = float(np.trace(np.linalg.solve(S_W_reg, S_B)))\n",
    "    score = J / (1.0 + J) if J >= 0 else 0.0\n",
    "\n",
    "    if not return_diagnostics:\n",
    "        return score\n",
    "\n",
    "    diagnostics = {\n",
    "        \"d\": d,\n",
    "        \"used_conditions\": list(groups.keys()),\n",
    "        \"ns\": dict(ns),\n",
    "        \"group_means\": {k: mus[k] for k in groups},\n",
    "        \"S_B\": S_B,\n",
    "        \"S_W\": S_W,\n",
    "        \"S_W_reg\": S_W_reg,\n",
    "        \"J\": J,\n",
    "        \"scaled\": bool(scale),\n",
    "        \"metrics\": list(metrics) if metrics is not None else \"all\",\n",
    "        \"reg\": eps if reg is None else reg,\n",
    "    }\n",
    "    return score, diagnostics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4894f518-7388-41a0-b694-eca8869ae274",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_G['AVA'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e9d650-7235-4e1e-ac7e-b16a09cf793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = variability_score(dict_G['AVA'], \n",
    "                           ignore_conditions=['scrambled', 'gfp'], \n",
    "                           metrics=[0,1,2],\n",
    "                           return_diagnostics=True)  \n",
    "# -> (total, dir, mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1ddb4-e2cc-437e-b125-30b088d300a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 'AVA'\n",
    "\n",
    "score, diag = variability_score_location_only(\n",
    "    dict_G[nc],\n",
    "    ignore_conditions=[\"scrambled\", \"gfp\"],\n",
    "    metrics=[0, 1, 2],\n",
    "    scale=True,\n",
    "    return_diagnostics=True,\n",
    ")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f9bb1-9eb3-4ccb-9f13-296e6e6fb6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Private CEPNEM JAX",
   "language": "python",
   "name": "private_cepnem_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
